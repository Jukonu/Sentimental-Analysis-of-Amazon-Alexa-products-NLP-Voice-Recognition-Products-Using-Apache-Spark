{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First configure our spark shell on yarn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_cores=2\n",
    "launcher.executor_memory='6000m'\n",
    "launcher.packages=[\"com.github.master:spark-stemming_2.10:0.2.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and Exploring Data. That have copied the data(amazon_alexa.tsv) to hdfs, loading the data in spark, see the schema, and print a few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bd-hm:8088/proxy/application_1575160893525_0011\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = yarn, app id = application_1575160893525_0011)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------------+--------------------+--------+\n",
      "|rating|     date|       variation|    verified_reviews|feedback|\n",
      "+------+---------+----------------+--------------------+--------+\n",
      "|     5|31-Jul-18|Charcoal Fabric |       Love my Echo!|       1|\n",
      "|     5|31-Jul-18|Charcoal Fabric |           Loved it!|       1|\n",
      "|     4|31-Jul-18|  Walnut Finish |Sometimes while p...|       1|\n",
      "+------+---------+----------------+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- variation: string (nullable = true)\n",
      " |-- verified_reviews: string (nullable = true)\n",
      " |-- feedback: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reviewsDF: org.apache.spark.sql.DataFrame = [rating: int, date: string ... 3 more fields]\n",
       "res0: Long = 3150\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reviewsDF=spark.read.option(\"header\",\"true\").option(\"delimiter\",\"\\t\").option(\"inferschema\", \"true\").option(\"escape\",\"\\\"\").csv(\"/hadoop-user/data/amazon_alexa.tsv\")\n",
    "reviewsDF.cache()\n",
    "reviewsDF.show(3)\n",
    "reviewsDF.printSchema()\n",
    "reviewsDF.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a quick view of this categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------------+--------------------+--------+\n",
      "|rating|     date|       variation|    verified_reviews|feedback|\n",
      "+------+---------+----------------+--------------------+--------+\n",
      "|     5|31-Jul-18|Charcoal Fabric |       Love my Echo!|       1|\n",
      "|     5|31-Jul-18|Charcoal Fabric |           Loved it!|       1|\n",
      "|     4|31-Jul-18|  Walnut Finish |Sometimes while p...|       1|\n",
      "+------+---------+----------------+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviewsDF.createOrReplaceTempView(\"reviews\")\n",
    "reviewsDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of feedback, variation and verified_reviews from their dataframe, reviewDF and viewing table output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|           variation|count(variation)|\n",
      "+--------------------+----------------+\n",
      "|Heather Gray Fabric |             157|\n",
      "|          Black  Dot|             516|\n",
      "|         Oak Finish |              14|\n",
      "|Configuration: Fi...|             350|\n",
      "|   Sandstone Fabric |              90|\n",
      "|         White  Show|              85|\n",
      "|         White  Plus|              78|\n",
      "|         White  Spot|             109|\n",
      "|         Black  Spot|             241|\n",
      "|         Black  Show|             265|\n",
      "|      Walnut Finish |               9|\n",
      "|               White|              91|\n",
      "|    Charcoal Fabric |             430|\n",
      "|          White  Dot|             184|\n",
      "|         Black  Plus|             270|\n",
      "|               Black|             261|\n",
      "+--------------------+----------------+\n",
      "\n",
      "+--------+---------------+\n",
      "|feedback|count(feedback)|\n",
      "+--------+---------------+\n",
      "|       1|           2893|\n",
      "|       0|            257|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select variation, count(variation) from reviews group by variation\").show(50)\n",
    "//spark.sql(\"select verified_reviews, count(verified_reviews) from reviews group by verified_reviews\").show(3)\n",
    "spark.sql(\"select feedback, count(feedback) from reviews group by feedback\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing the unordered categorical variables, variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           variation|\n",
      "+--------------------+\n",
      "|     Charcoal Fabric|\n",
      "|       Walnut Finish|\n",
      "| Heather Gray Fabric|\n",
      "|          Oak Finish|\n",
      "|    Sandstone Fabric|\n",
      "|Configuration: Fi...|\n",
      "|         White  Show|\n",
      "|         White  Plus|\n",
      "|         White  Spot|\n",
      "|         Black  Spot|\n",
      "|         Black  Show|\n",
      "|               White|\n",
      "|          White  Dot|\n",
      "|         Black  Plus|\n",
      "|           Black Dot|\n",
      "|               Black|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "df: org.apache.spark.sql.DataFrame = [variation: string]\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_2fba33d92ceb\n",
       "indexed: org.apache.spark.sql.DataFrame = [variation: string, index: double]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature._\n",
    "val df=Seq((\"Charcoal Fabric\"),(\"Walnut Finish\"),(\"Heather Gray Fabric\"),(\"Oak Finish\"),(\"Sandstone Fabric\"),\n",
    "          (\"Configuration: Fire TV Stick\"),(\"White  Show\"),(\"White  Plus\"),(\"White  Spot\"),(\"Black  Spot\"),\n",
    "          (\"Black  Show\"),(\"White\"),(\"White  Dot\"),(\"Black  Plus\"),(\"Black Dot\"),(\"Black\")).toDF(\"variation\")\n",
    "df.show()\n",
    "val indexer=new StringIndexer().setOutputCol(\"index\").setInputCol(\"variation\")\n",
    "val indexed=indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneHotEncoding the index variable, variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---------------+\n",
      "|           variation|index|       codedvec|\n",
      "+--------------------+-----+---------------+\n",
      "|     Charcoal Fabric|  2.0| (15,[2],[1.0])|\n",
      "|       Walnut Finish| 12.0|(15,[12],[1.0])|\n",
      "| Heather Gray Fabric|  9.0| (15,[9],[1.0])|\n",
      "|          Oak Finish|  0.0| (15,[0],[1.0])|\n",
      "|    Sandstone Fabric| 13.0|(15,[13],[1.0])|\n",
      "|Configuration: Fi...|  3.0| (15,[3],[1.0])|\n",
      "|         White  Show| 11.0|(15,[11],[1.0])|\n",
      "|         White  Plus| 14.0|(15,[14],[1.0])|\n",
      "|         White  Spot|  5.0| (15,[5],[1.0])|\n",
      "|         Black  Spot|  7.0| (15,[7],[1.0])|\n",
      "|         Black  Show|  1.0| (15,[1],[1.0])|\n",
      "|               White|  4.0| (15,[4],[1.0])|\n",
      "|          White  Dot| 10.0|(15,[10],[1.0])|\n",
      "|         Black  Plus|  6.0| (15,[6],[1.0])|\n",
      "|           Black Dot| 15.0|     (15,[],[])|\n",
      "|               Black|  8.0| (15,[8],[1.0])|\n",
      "+--------------------+-----+---------------+\n",
      "\n",
      "root\n",
      " |-- variation: string (nullable = true)\n",
      " |-- index: double (nullable = false)\n",
      " |-- codedvec: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_36a441c88949\n",
       "encoded: org.apache.spark.sql.DataFrame = [variation: string, index: double ... 1 more field]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val encoder= new OneHotEncoderEstimator().setInputCols(Array(\"index\")).setOutputCols(Array(\"codedvec\"))\n",
    "val encoded=encoder.fit(indexed).transform(indexed)\n",
    "encoded.show()\n",
    "encoded.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the weighted logistic regression to generate weighteddatase since we have an imbalanced data where the minority id far less than the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------------+--------------------+--------+-------------------+\n",
      "|rating|     date|       variation|    verified_reviews|feedback|     classWeightCol|\n",
      "+------+---------+----------------+--------------------+--------+-------------------+\n",
      "|     5|31-Jul-18|Charcoal Fabric |       Love my Echo!|       1|0.08158730158730154|\n",
      "|     5|31-Jul-18|Charcoal Fabric |           Loved it!|       1|0.08158730158730154|\n",
      "|     4|31-Jul-18|  Walnut Finish |Sometimes while p...|       1|0.08158730158730154|\n",
      "|     5|31-Jul-18|Charcoal Fabric |I have had a lot ...|       1|0.08158730158730154|\n",
      "|     5|31-Jul-18|Charcoal Fabric |               Music|       1|0.08158730158730154|\n",
      "+------+---------+----------------+--------------------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numNegatives: Long = 257\n",
       "datasetSize: Long = 3150\n",
       "balancingRatio: Double = 0.9184126984126985\n",
       "calculateWeights: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,DoubleType,Some(List(DoubleType)))\n",
       "weightedDataset: org.apache.spark.sql.DataFrame = [rating: int, date: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    // Re-balancing (weighting) of records to be used in the logistic loss objective function\n",
    "    val numNegatives = reviewsDF.filter(reviewsDF(\"feedback\") === 0).count\n",
    "    val datasetSize = reviewsDF.count\n",
    "    val balancingRatio = (datasetSize - numNegatives).toDouble / datasetSize\n",
    "\n",
    "    val calculateWeights = udf { feedback: Double =>\n",
    "      if (feedback == 0.0) {\n",
    "        1 * balancingRatio\n",
    "      }\n",
    "      else {\n",
    "        (1 * (1.0 - balancingRatio))\n",
    "      }\n",
    "    }\n",
    "\n",
    "    val weightedDataset = reviewsDF.withColumn(\"classWeightCol\", calculateWeights(reviewsDF(\"feedback\")))\n",
    "    weightedDataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering processes: Tokenizing, Removing Stop Words and Stemming, Vectorizing words using CountVectorizer, Vector Assembler, indexer, encoder, Generating TFIDF vectors, vectorizer_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_1a99d07c7083\n",
       "import org.apache.spark.sql.functions.udf\n",
       "import org.apache.spark.sql.DataFrame\n",
       "removePunc: (words: Seq[String])Seq[String]\n",
       "puncRemover: org.apache.spark.ml.feature.SQLTransformer = sql_e9d17e889d9f\n",
       "stopWordRemover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_f99d4c4e278c\n",
       "import org.apache.spark.mllib.feature.Stemmer\n",
       "stemmer: org.apache.spark.mllib.feature.Stemmer = stemmer_e5e76c7b9f08\n",
       "vectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_6c1b3833c3d4\n",
       "tfidf: org.apache.spark.ml.feature.IDF = idf_2ef2f0451390\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_49ccaab31863\n",
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEn..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "val tokenizer = new RegexTokenizer().setMinTokenLength(2).setToLowercase(true).setInputCol(\"verified_reviews\").setOutputCol(\"words\")\n",
    "\n",
    "//Defining a udf to remove punctuations from a sequence of words\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "//df1 = df.withColumnRenamed('sum(\"session\")', \"session\")\n",
    "//val reviewerDF=reviewsDF.withColumn(\"classWeightCol\", reviewsDF(\"classWeightCol\"))\n",
    "\n",
    "def removePunc(words:Seq[String]):Seq[String]={\n",
    " return words.map(_.replaceAll(\"\\\\p{Punct}\",\" \"))\n",
    "}\n",
    "\n",
    "//val removePuncUDF=udf(removePunc(_:Seq[String]))\n",
    "spark.udf.register(\"removePuncUDF\",removePunc(_:Seq[String]) )\n",
    "\n",
    "val puncRemover = new SQLTransformer().setStatement(\"SELECT removePuncUDF(words) as words,variation,classWeightCol,feedback from __THIS__ \")\n",
    "\n",
    "val stopWordRemover=new StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered_words\")\n",
    "\n",
    "import org.apache.spark.mllib.feature.Stemmer\n",
    "val stemmer = new Stemmer().setInputCol(\"filtered_words\").setOutputCol(\"stemmed_words\")\n",
    "\n",
    "val vectorizer = new CountVectorizer().setMinDF(100).setInputCol(\"stemmed_words\").setOutputCol(\"stemmed_BOW\")\n",
    "\n",
    "val tfidf = new IDF().setInputCol(\"stemmed_BOW\").setOutputCol(\"reviews_TFIDF\") \n",
    "\n",
    "val indexer=new StringIndexer().setOutputCol(\"index\").setInputCol(\"variation\")\n",
    "\n",
    "val encoder= new OneHotEncoderEstimator().setInputCols(Array(\"index\")).setOutputCols(Array(\"codedvec\"))\n",
    "\n",
    "val vectorizer_all=new VectorAssembler().setInputCols(Array(\"codedvec\",\"reviews_TFIDF\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building, Tunning, and Evaluating a weighted Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+--------------------+\n",
      "|feedback|prediction|         probability|       stemmed_words|\n",
      "+--------+----------+--------------------+--------------------+\n",
      "|       0|       0.0|[0.84551088820414...|[item, never, wor...|\n",
      "|       0|       0.0|[0.79036624675199...|[cant, seem, get,...|\n",
      "|       0|       1.0|[0.29785282846358...|[love, product, d...|\n",
      "|       0|       0.0|[0.70373145846611...|                  []|\n",
      "|       0|       0.0|[0.96318669237066...|[can t, turn,   3...|\n",
      "+--------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC curve(AUC) for LR on test data = 0.8147730526918672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_1dd746a0b885\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_1dd746a0b885-elasticNetParam: 0.0,\n",
       "\tidf_2ef2f0451390-minDocFreq: 5,\n",
       "\tlogreg_1dd746a0b885-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_1dd746a0b885-elasticNetParam: 0.0,\n",
       "\tidf_2ef2f0451390-minDocFreq: 10,\n",
       "\tlogreg_1dd746a0b885-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_1dd746a0b885-elasticNetParam: 0.5,\n",
       "\tidf_2ef2f0451390-minDocFreq: 5,\n",
       "\tlogreg_1dd746a0b885-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_1dd746a0b885-elasticNetParam: 0.5,\n",
       "\tidf_2ef2f0451390-minDocFreq: 10..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "val lr = new LogisticRegression().setWeightCol(\"classWeightCol\").setLabelCol(\"feedback\").setFeaturesCol(\"features\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, Array(0.01, 0.5, 2.0))\n",
    "             .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "             .addGrid(tfidf.minDocFreq, Array(5,10))\n",
    "             .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"feedback\").setMetricName(\"areaUnderROC\")\n",
    "val cv = new CrossValidator().setEstimator(lr).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(5)\n",
    "\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer,puncRemover,stopWordRemover, stemmer,vectorizer,tfidf,indexer, encoder,vectorizer_all, cv))\n",
    "\n",
    "val Array(training,testing)=weightedDataset.randomSplit(Array(0.8,0.2),111)\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val pipelineModel = pipeline.fit(training)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = pipelineModel.transform(testing)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"feedback\", \"prediction\", \"probability\", \"stemmed_words\").show(5)\n",
    "\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC curve(AUC) for LR on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Feature engineering processes while removing the weighted classColumn: Tokenizing, Removing Stop Words and Stemming, Vectorizing words using CountVectorizer, Vector Assembler, indexer, encoder, Generating TFIDF vectors, vectorizer_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_10a4c5665649\n",
       "import org.apache.spark.sql.functions.udf\n",
       "import org.apache.spark.sql.DataFrame\n",
       "removePunc: (words: Seq[String])Seq[String]\n",
       "puncRemover: org.apache.spark.ml.feature.SQLTransformer = sql_afac762c6feb\n",
       "stopWordRemover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_fd85ce1019cc\n",
       "import org.apache.spark.mllib.feature.Stemmer\n",
       "stemmer: org.apache.spark.mllib.feature.Stemmer = stemmer_a45077bb5be6\n",
       "vectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_eeac5fb05921\n",
       "tfidf: org.apache.spark.ml.feature.IDF = idf_1422f5b60202\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_35b115fe9888\n",
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEn..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "val tokenizer = new RegexTokenizer().setMinTokenLength(2).setToLowercase(true).setInputCol(\"verified_reviews\").setOutputCol(\"words\")\n",
    "\n",
    "//Defining a udf to remove punctuations from a sequence of words\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def removePunc(words:Seq[String]):Seq[String]={\n",
    " return words.map(_.replaceAll(\"\\\\p{Punct}\",\" \"))\n",
    "}\n",
    "\n",
    "//val removePuncUDF=udf(removePunc(_:Seq[String]))\n",
    "spark.udf.register(\"removePuncUDF\",removePunc(_:Seq[String]) )\n",
    "\n",
    "val puncRemover = new SQLTransformer().setStatement(\"SELECT removePuncUDF(words) as words,variation,feedback from __THIS__ \")\n",
    "\n",
    "val stopWordRemover=new StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered_words\")\n",
    "\n",
    "import org.apache.spark.mllib.feature.Stemmer\n",
    "val stemmer = new Stemmer().setInputCol(\"filtered_words\").setOutputCol(\"stemmed_words\")\n",
    "\n",
    "val vectorizer = new CountVectorizer().setMinDF(100).setInputCol(\"stemmed_words\").setOutputCol(\"stemmed_BOW\")\n",
    "\n",
    "val tfidf = new IDF().setInputCol(\"stemmed_BOW\").setOutputCol(\"reviews_TFIDF\") \n",
    "\n",
    "val indexer=new StringIndexer().setOutputCol(\"index\").setInputCol(\"variation\")\n",
    "\n",
    "val encoder= new OneHotEncoderEstimator().setInputCols(Array(\"index\")).setOutputCols(Array(\"codedvec\"))\n",
    "\n",
    "val vectorizer_all=new VectorAssembler().setInputCols(Array(\"codedvec\",\"reviews_TFIDF\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPlitting the DataFrame ReviewsDF into the Training and Testing Data and seeding at 111."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [rating: int, date: string ... 3 more fields]\n",
       "testing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [rating: int, date: string ... 3 more fields]\n",
       "res7: Long = 2520\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training,testing)=reviewsDF.randomSplit(Array(0.8,0.2),111)\n",
    "training.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UpSampling the splitted training Data due imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+--------------------+--------+\n",
      "|rating|    date|variation|    verified_reviews|feedback|\n",
      "+------+--------+---------+--------------------+--------+\n",
      "|     1|1-Jul-18|    White|This item did not...|       0|\n",
      "|     1|1-Jul-18|    White|This item did not...|       0|\n",
      "|     1|1-Jul-18|    White|This item did not...|       0|\n",
      "+------+--------+---------+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "2116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newMinorclass: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [rating: int, date: string ... 3 more fields]\n",
       "deMinorsDF: Long = 209\n",
       "noOfSamples: Int = 20\n",
       "minsampleDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [rating: int, date: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newMinorclass=training.filter(training(\"feedback\") === 0)\n",
    "val deMinorsDF=newMinorclass.count()\n",
    "val noOfSamples = 20\n",
    "var minsampleDF = newMinorclass.sample(true, 1D*deMinorsDF / noOfSamples)\n",
    "minsampleDF.show(3)\n",
    "println(minsampleDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingsDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [rating: int, date: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingsDF = training.union(minsampleDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing DataFrame while selecting rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|feedback|count(feedback)|\n",
      "+--------+---------------+\n",
      "|       1|           2311|\n",
      "|       0|           2325|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingsDF.createOrReplaceTempView(\"trainings\")\n",
    "spark.sql(\"select feedback,count(feedback) from trainings group by feedback \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building, Tunning, and Evaluating a RandomForest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+--------------------+\n",
      "|feedback|prediction|         probability|       stemmed_words|\n",
      "+--------+----------+--------------------+--------------------+\n",
      "|       0|       0.0|[0.59859082272866...|[item, never, wor...|\n",
      "|       0|       0.0|[0.73957681464467...|[cant, seem, get,...|\n",
      "|       0|       1.0|[0.39954812810485...|[love, product, d...|\n",
      "|       0|       0.0|[0.53617399265660...|                  []|\n",
      "|       0|       0.0|[0.60464527348659...|[can t, turn,   3...|\n",
      "+--------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC curve(AUC) for RF on test data = 0.810889175257732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.RandomForestClassifier\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_a2b963fe9922\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfc_a2b963fe9922-maxDepth: 2,\n",
       "\trfc_a2b963fe9922-numTrees: 5\n",
       "}, {\n",
       "\trfc_a2b963fe9922-maxDepth: 2,\n",
       "\trfc_a2b963fe9922-numTrees: 20\n",
       "}, {\n",
       "\trfc_a2b963fe9922-maxDepth: 5,\n",
       "\trfc_a2b963fe9922-numTrees: 5\n",
       "}, {\n",
       "\trfc_a2b963fe9922-maxDepth: 5,\n",
       "\trfc_a2b963fe9922-numTrees: 20\n",
       "})\n",
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_656f673ba702\n",
       "cv_rf: org.apache.spark.ml.tuning.CrossValidator = cv_edf1a820d..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"feedback\").setFeaturesCol(\"features\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, Array(2, 5))\n",
    "             .addGrid(rf.numTrees, Array(5, 20))\n",
    "             .build()\n",
    "\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"feedback\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "val cv_rf = new CrossValidator().setEstimator(rf).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "val pipeline_rf = new Pipeline().setStages(Array(tokenizer,puncRemover,stopWordRemover, stemmer,vectorizer,tfidf,indexer, encoder,vectorizer_all, cv_rf))\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val pipelineModel_rf = pipeline_rf.fit(trainingsDF)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = pipelineModel_rf.transform(testing)\n",
    "\n",
    "predictions.select(\"feedback\", \"prediction\", \"probability\", \"stemmed_words\").show(5) \n",
    "\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC curve(AUC) for RF on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building, Tunning, and Evaluating a GBT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+--------------------+\n",
      "|feedback|prediction|         probability|       stemmed_words|\n",
      "+--------+----------+--------------------+--------------------+\n",
      "|       0|       0.0|[0.59454606810298...|[item, never, wor...|\n",
      "|       0|       0.0|[0.95670287869975...|[cant, seem, get,...|\n",
      "|       0|       1.0|[0.03149227055179...|[love, product, d...|\n",
      "|       0|       0.0|[0.67108377333154...|                  []|\n",
      "|       0|       1.0|[0.34449974663627...|[can t, turn,   3...|\n",
      "+--------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC curve(AUC) for GBT on test data = 0.8274806701030928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "gbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_3be9670c32ba\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tgbtc_3be9670c32ba-maxDepth: 2,\n",
       "\tgbtc_3be9670c32ba-maxIter: 10\n",
       "}, {\n",
       "\tgbtc_3be9670c32ba-maxDepth: 2,\n",
       "\tgbtc_3be9670c32ba-maxIter: 20\n",
       "}, {\n",
       "\tgbtc_3be9670c32ba-maxDepth: 2,\n",
       "\tgbtc_3be9670c32ba-maxIter: 100\n",
       "}, {\n",
       "\tgbtc_3be9670c32ba-maxDepth: 5,\n",
       "\tgbtc_3be9670c32ba-maxIter: 10\n",
       "}, {\n",
       "\tgbtc_3be9670c32ba-maxDepth: 5,\n",
       "\tgbtc_3be9670c32ba-maxIter: 20\n",
       "}, {\n",
       "\tgbtc_3be9670c32ba-maxDepth: 5,\n",
       "\tgbtc_3be9670c32ba-maxIter: 100\n",
       "})\n",
       "eval..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "// Create a GBT model.\n",
    "val gbt = new GBTClassifier().setLabelCol(\"feedback\").setFeaturesCol(\"features\")\n",
    "//Create ParamGrid for Cross Validation\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, Array(2,5))\n",
    "             .addGrid(gbt.maxIter, Array(10, 20,100))\n",
    "             .build()\n",
    "\n",
    "val evaluator = new BinaryClassificationEvaluator()\n",
    "  .setRawPredictionCol(\"rawPrediction\")\n",
    "  .setLabelCol(\"feedback\")\n",
    "  .setMetricName(\"areaUnderROC\")\n",
    "\n",
    "val cv = new CrossValidator().setEstimator(gbt).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer,puncRemover,stopWordRemover, stemmer,vectorizer,tfidf,indexer, encoder,vectorizer_all, cv))\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val pipelineModel = pipeline.fit(trainingsDF)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = pipelineModel.transform(testing)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"feedback\", \"prediction\", \"probability\", \"stemmed_words\").show(5)\n",
    "\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC curve(AUC) for GBT on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
